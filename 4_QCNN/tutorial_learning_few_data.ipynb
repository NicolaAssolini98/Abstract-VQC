{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nimport seaborn as sns\nimport jax;\n\njax.config.update('jax_platform_name', 'cpu')\njax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\n\nimport optax  # optimization using jax\n\nimport pennylane as qml\nimport pennylane.numpy as pnp\n\nsns.set()\n\nseed = 0\nrng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convolutional_layer(weights, wires, skip_first_layer=True):\n    \"\"\"Adds a convolutional layer to a circuit.\n    Args:\n        weights (np.array): 1D array with 15 weights of the parametrized gates.\n        wires (list[int]): Wires where the convolutional layer acts on.\n        skip_first_layer (bool): Skips the first two U3 gates of a layer.\n    \"\"\"\n    n_wires = len(wires)\n    assert n_wires >= 3, \"this circuit is too small!\"\n\n    for p in [0, 1]:\n        for indx, w in enumerate(wires):\n            if indx % 2 == p and indx < n_wires - 1:\n                if indx % 2 == 0 and not skip_first_layer:\n                    qml.U3(*weights[:3], wires=[w])\n                    qml.U3(*weights[3:6], wires=[wires[indx + 1]])\n                qml.IsingXX(weights[6], wires=[w, wires[indx + 1]])\n                qml.IsingYY(weights[7], wires=[w, wires[indx + 1]])\n                qml.IsingZZ(weights[8], wires=[w, wires[indx + 1]])\n                qml.U3(*weights[9:12], wires=[w])\n                qml.U3(*weights[12:], wires=[wires[indx + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pooling_layer(weights, wires):\n    \"\"\"Adds a pooling layer to a circuit.\n    Args:\n        weights (np.array): Array with the weights of the conditional U3 gate.\n        wires (list[int]): List of wires to apply the pooling layer on.\n    \"\"\"\n    n_wires = len(wires)\n    assert len(wires) >= 2, \"this circuit is too small!\"\n\n    for indx, w in enumerate(wires):\n        if indx % 2 == 1 and indx < n_wires:\n            m_outcome = qml.measure(w)\n            qml.cond(m_outcome, qml.U3)(*weights, wires=wires[indx - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_and_pooling(kernel_weights, n_wires, skip_first_layer=True):\n    \"\"\"Apply both the convolutional and pooling layer.\"\"\"\n    convolutional_layer(kernel_weights[:15], n_wires, skip_first_layer=skip_first_layer)\n    pooling_layer(kernel_weights[15:], n_wires)\n\n\ndef dense_layer(weights, wires):\n    \"\"\"Apply an arbitrary unitary gate to a specified set of wires.\"\"\"\n    qml.ArbitraryUnitary(weights, wires)\n\n\nnum_wires = 6\ndevice = qml.device(\"default.qubit\", wires=num_wires)\n\n\n@qml.qnode(device)\ndef conv_net(weights, last_layer_weights, features):\n    \"\"\"Define the QCNN circuit\n    Args:\n        weights (np.array): Parameters of the convolution and pool layers.\n        last_layer_weights (np.array): Parameters of the last dense layer.\n        features (np.array): Input data to be embedded using AmplitudEmbedding.\"\"\"\n\n    layers = weights.shape[1]\n    wires = list(range(num_wires))\n\n    # inputs the state input_state\n    qml.AmplitudeEmbedding(features=features, wires=wires, pad_with=0.5)\n    qml.Barrier(wires=wires, only_visual=True)\n\n    # adds convolutional and pooling layers\n    for j in range(layers):\n        conv_and_pooling(weights[:, j], wires, skip_first_layer=(not j == 0))\n        wires = wires[::2]\n        qml.Barrier(wires=wires, only_visual=True)\n\n    assert last_layer_weights.size == 4 ** (len(wires)) - 1, (\n        \"The size of the last layer weights vector is incorrect!\"\n        f\" \\n Expected {4 ** (len(wires)) - 1}, Given {last_layer_weights.size}\"\n    )\n    dense_layer(last_layer_weights, wires)\n    return qml.probs(wires=(0))\n\n\nfig, ax = qml.draw_mpl(conv_net)(\n    np.random.rand(18, 2), np.random.rand(4 ** 2 - 1), np.random.rand(2 ** num_wires)\n)\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\nimages, labels = digits.data, digits.target\n\nimages = images[np.where((labels == 0) | (labels == 1))]\nlabels = labels[np.where((labels == 0) | (labels == 1))]\n\nfig, axes = plt.subplots(nrows=1, ncols=12, figsize=(3, 1))\n\nfor i, ax in enumerate(axes.flatten()):\n    ax.imshow(images[i].reshape((8, 8)), cmap=\"gray\")\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_digits_data(num_train, num_test, rng):\n    \"\"\"Return training and testing data of digits dataset.\"\"\"\n    digits = datasets.load_digits()\n    features, labels = digits.data, digits.target\n\n    # only use first two classes\n    features = features[np.where((labels == 0) | (labels == 1))]\n    labels = labels[np.where((labels == 0) | (labels == 1))]\n\n    # normalize data\n    features = features / np.linalg.norm(features, axis=1).reshape((-1, 1))\n\n    # subsample train and test split\n    train_indices = rng.choice(len(labels), num_train, replace=False)\n    test_indices = rng.choice(\n        np.setdiff1d(range(len(labels)), train_indices), num_test, replace=False\n    )\n\n    x_train, y_train = features[train_indices], labels[train_indices]\n    x_test, y_test = features[test_indices], labels[test_indices]\n\n    return (\n        jnp.asarray(x_train),\n        jnp.asarray(y_train),\n        jnp.asarray(x_test),\n        jnp.asarray(y_test),\n    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\ndef compute_out(weights, weights_last, features, labels):\n    \"\"\"Computes the output of the corresponding label in the qcnn\"\"\"\n    cost = lambda weights, weights_last, feature, label: conv_net(weights, weights_last, feature)[\n        label\n    ]\n    return jax.vmap(cost, in_axes=(None, None, 0, 0), out_axes=0)(\n        weights, weights_last, features, labels\n    )\n\n\ndef compute_accuracy(weights, weights_last, features, labels):\n    \"\"\"Computes the accuracy over the provided features and labels\"\"\"\n    out = compute_out(weights, weights_last, features, labels)\n    return jnp.sum(out > 0.5) / len(out)\n\n\ndef compute_cost(weights, weights_last, features, labels):\n    \"\"\"Computes the cost over the provided features and labels\"\"\"\n    out = compute_out(weights, weights_last, features, labels)\n    return 1.0 - jnp.sum(out) / len(labels)\n\n\ndef init_weights():\n    \"\"\"Initializes random weights for the QCNN model.\"\"\"\n    weights = pnp.random.normal(loc=0, scale=1, size=(18, 2), requires_grad=True)\n    weights_last = pnp.random.normal(loc=0, scale=1, size=4 ** 2 - 1, requires_grad=True)\n    return jnp.array(weights), jnp.array(weights_last)\n\n\nvalue_and_grad = jax.jit(jax.value_and_grad(compute_cost, argnums=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_qcnn(n_train, n_test, n_epochs):\n    \"\"\"\n    Args:\n        n_train  (int): number of training examples\n        n_test   (int): number of test examples\n        n_epochs (int): number of training epochs\n        desc  (string): displayed string during optimization\n\n    Returns:\n        dict: n_train,\n        steps,\n        train_cost_epochs,\n        train_acc_epochs,\n        test_cost_epochs,\n        test_acc_epochs\n\n    \"\"\"\n    # load data\n    x_train, y_train, x_test, y_test = load_digits_data(n_train, n_test, rng)\n\n    # init weights and optimizer\n    weights, weights_last = init_weights()\n\n    # learning rate decay\n    cosine_decay_scheduler = optax.cosine_decay_schedule(0.1, decay_steps=n_epochs, alpha=0.95)\n    optimizer = optax.adam(learning_rate=cosine_decay_scheduler)\n    opt_state = optimizer.init((weights, weights_last))\n\n    # data containers\n    train_cost_epochs, test_cost_epochs, train_acc_epochs, test_acc_epochs = [], [], [], []\n\n    for step in range(n_epochs):\n        # Training step with (adam) optimizer\n        train_cost, grad_circuit = value_and_grad(weights, weights_last, x_train, y_train)\n        updates, opt_state = optimizer.update(grad_circuit, opt_state)\n        weights, weights_last = optax.apply_updates((weights, weights_last), updates)\n\n        train_cost_epochs.append(train_cost)\n\n        # compute accuracy on training data\n        train_acc = compute_accuracy(weights, weights_last, x_train, y_train)\n        train_acc_epochs.append(train_acc)\n\n        # compute accuracy and cost on testing data\n        test_out = compute_out(weights, weights_last, x_test, y_test)\n        test_acc = jnp.sum(test_out > 0.5) / len(test_out)\n        test_acc_epochs.append(test_acc)\n        test_cost = 1.0 - jnp.sum(test_out) / len(test_out)\n        test_cost_epochs.append(test_cost)\n\n    return dict(\n        n_train=[n_train] * n_epochs,\n        step=np.arange(1, n_epochs + 1, dtype=int),\n        train_cost=train_cost_epochs,\n        train_acc=train_acc_epochs,\n        test_cost=test_cost_epochs,\n        test_acc=test_acc_epochs,\n    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_test = 100\nn_epochs = 100\nn_reps = 100\n\n\ndef run_iterations(n_train):\n    results_df = pd.DataFrame(\n        columns=[\"train_acc\", \"train_cost\", \"test_acc\", \"test_cost\", \"step\", \"n_train\"]\n    )\n\n    for _ in range(n_reps):\n        results = train_qcnn(n_train=n_train, n_test=n_test, n_epochs=n_epochs)\n        results_df = pd.concat(\n            [results_df, pd.DataFrame.from_dict(results)], axis=0, ignore_index=True\n        )\n\n    return results_df\n\n\n# run training for multiple sizes\ntrain_sizes = [2, 5, 10, 20, 40, 80]\nresults_df = run_iterations(n_train=2)\nfor n_train in train_sizes[1:]:\n    results_df = pd.concat([results_df, run_iterations(n_train=n_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregate dataframe\ndf_agg = results_df.groupby([\"n_train\", \"step\"]).agg([\"mean\", \"std\"])\ndf_agg = df_agg.reset_index()\n\nsns.set_style('whitegrid')\ncolors = sns.color_palette()\nfig, axes = plt.subplots(ncols=3, figsize=(16.5, 5))\n\ngeneralization_errors = []\n\n# plot losses and accuracies\nfor i, n_train in enumerate(train_sizes):\n    df = df_agg[df_agg.n_train == n_train]\n\n    dfs = [df.train_cost[\"mean\"], df.test_cost[\"mean\"], df.train_acc[\"mean\"], df.test_acc[\"mean\"]]\n    lines = [\"o-\", \"x--\", \"o-\", \"x--\"]\n    labels = [fr\"$N={n_train}$\", None, fr\"$N={n_train}$\", None]\n    axs = [0,0,2,2]\n    \n    for k in range(4):\n        ax = axes[axs[k]]   \n        ax.plot(df.step, dfs[k], lines[k], label=labels[k], markevery=10, color=colors[i], alpha=0.8)\n\n\n    # plot final loss difference\n    dif = df[df.step == 100].test_cost[\"mean\"] - df[df.step == 100].train_cost[\"mean\"]\n    generalization_errors.append(dif)\n\n# format loss plot\nax = axes[0]\nax.set_title('Train and Test Losses', fontsize=14)\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\n\n# format generalization error plot\nax = axes[1]\nax.plot(train_sizes, generalization_errors, \"o-\", label=r\"$gen(\\alpha)$\")\nax.set_xscale('log')\nax.set_xticks(train_sizes)\nax.set_xticklabels(train_sizes)\nax.set_title(r'Generalization Error $gen(\\alpha) = R(\\alpha) - \\hat{R}_N(\\alpha)$', fontsize=14)\nax.set_xlabel('Training Set Size')\n\n# format loss plot\nax = axes[2]\nax.set_title('Train and Test Accuracies', fontsize=14)\nax.set_xlabel('Epoch')\nax.set_ylabel('Accuracy')\nax.set_ylim(0.5, 1.05)\n\nlegend_elements = [\n    mpl.lines.Line2D([0], [0], label=f'N={n}', color=colors[i]) for i, n in enumerate(train_sizes)\n    ] + [\n    mpl.lines.Line2D([0], [0], marker='o', ls='-', label='Train', color='Black'),\n    mpl.lines.Line2D([0], [0], marker='x', ls='--', label='Test', color='Black')\n    ]\n\naxes[0].legend(handles=legend_elements, ncol=3)\naxes[2].legend(handles=legend_elements, ncol=3)\n\naxes[1].set_yscale('log', base=2)\nplt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
