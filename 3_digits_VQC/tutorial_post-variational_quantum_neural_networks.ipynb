{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T17:16:15.069697Z",
     "start_time": "2025-07-04T17:16:14.781745Z"
    }
   },
   "source": [
    "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T17:16:17.530944Z",
     "start_time": "2025-07-04T17:16:16.083543Z"
    }
   },
   "source": [
    "import pennylane as qml\nfrom pennylane import numpy as np\nimport jax\nfrom jax import numpy as jnp\nimport optax\nfrom itertools import combinations\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n# Load the digits dataset with features (X_digits) and labels (y_digits)\nX_digits, y_digits = load_digits(return_X_y=True)\n\n# Create a boolean mask to filter out only the samples where the label is 2 or 6\nfilter_mask = np.isin(y_digits, [2, 6])\n\n# Apply the filter mask to the features and labels to keep only the selected digits\nX_digits = X_digits[filter_mask]\ny_digits = y_digits[filter_mask]\n\n# Split the filtered dataset into training and testing sets with 10% of data reserved for testing\nX_train, X_test, y_train, y_test = train_test_split(\n    X_digits, y_digits, test_size=0.1, random_state=42\n)\n\n# Normalize the pixel values in the training and testing data\n# Convert each image from a 1D array to an 8x8 2D array, normalize pixel values, and scale them\nX_train = np.array([thing.reshape([8, 8]) / 16 * 2 * np.pi for thing in X_train])\nX_test = np.array([thing.reshape([8, 8]) / 16 * 2 * np.pi for thing in X_test])\n\n# Adjust the labels to be centered around 0 and scaled to be in the range -1 to 1\n# The original labels (2 and 6) are mapped to -1 and 1 respectively\ny_train = (y_train - 4) / 2\ny_test = (y_test - 4) / 2"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicola/miniconda3/envs/vvqc/lib/python3.12/site-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.6.2 is installed, but it is not compatible with the installed jaxlib version 0.5.3, so it will not be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T17:16:24.063285Z",
     "start_time": "2025-07-04T17:16:23.999196Z"
    }
   },
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, layout=\"constrained\")\nfor i in range(2):\n    for j in range(3):\n      axes[i][j].matshow(X_train[2*(2*j+i)])\n      axes[i][j].axis('off')\nfig.subplots_adjust(hspace=0.0)\nfig.tight_layout()\nplt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAGyCAYAAABpxYnGAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAADlNJREFUeJzt2F+o13cdx/G3nrPNzlgrcSoh0sqSIW0N+jfplAWrUYOOrLCyjQ6sVQtqp1qFXVgXuv7MhCBbf6VVsBhkQYxqFMjAo3Vhq4mcwiyplTYOsQ0vThxPV+tuODy9z+/4Oo/Htb4+n/35nvPks2xubm6uAAC46C0f9AUAAPj/EHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQYvi5/sEbl7+r8x4RpsdvaN2//D3/aN0fufVs6/7s6TOt+wvh4XMPXtDf6/5+htasbt3fc+QnrftVVddcOtK6v2lye+v+uluOte4nuNDvp6r/Gzr3xutb97/6va+17ldVTf2n9+fArt23tu6v3D/Zup/guXxDXuwAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIMTwoC+Q5LM7vj/oK8zLN1e9vfeA02d695ewpzZf3bp/zaUjrftVVVf//PbW/ZM3fbt1f3TrB1v3Rw4cad1f6j7y7Qdb96f+s7p1fyHcv3NP6/5d+ze37i8VXuwAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIMTwoC+wUKbHb2g/Y+zy37Xuv/qzH27dX3lssnWfPlf88d+t++OnRlv3q6pWPXJJ7wE39c6vmJ7pPYBWn/jZ+1r3N37hz637VVV/f++G1v1H797Xuj+0aWPr/uyxqdb9xcKLHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGGB32BhTJ97Vz7GcdnzrbuX/Wb6db92dZ1Os0em2rdf/x1rfNVVTUzsaz/EHgWGyYOt+4vxM/XJ1+xfgFOYbHzYgcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYYHfYGFsmL9U+1nHHjy+tb92WNTrfvwbIbWrG4/44GP3du6v/uJ3u9z+cGjrftc3BbiG9o7+kDr/pbHxlr3L/M77v/Cix0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBge9AUWyq5rf9p+xtjlT7fu73i8db5e+qMPte5vmDjcuk+fqc+8pP2May4dad2/Y+eW1v2ROtK6z8Xt+D3r288Yu/yXrfvfnLisdX+2db1qaM3q5hOqZk+faT/jfLzYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAECI4UFfYKHs3Hdb+xljd+9r3b/657e37j+09Sut++8+9cnW/aqqtXsPtZ+xGA1t2ti6f2Lbfa37VVW7n+j9Z+h2dutrW/dXTM+07ldVLT94tP2Mxar7v9/Jm77Rul9VdXzmbOv+1O0vbN0/d+WrWvcfevNXW/erqu568eb2M87Hix0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhgd9gYWydu+h9jOuqztb9/feeX/r/jWXjrTuP73+XOs+fXY/sbH9jB2rpnr3v9a7n+BtN24b9BUGZsX0zKCvMG/dP8NPbLuvdf/4zNnW/ds+/4nW/aqqlTXZfsb5eLEDAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDL5ubm5gZ9CQAA5s+LHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBh+rn/wxuXv6rxHhD9+91Wt+3tHH2jd3/Op7a37IweOtO4vhIfPPXhBf8/3c36nPre5df/H77+3df+uF/feP8GFfj9V/d/Q0JrVrfuvf/ivrftVVTtWTbWf0Wn0Ix9s3V8qv4O82AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGGB32BhfLPic3tZ5y8aV/r/vip0dZ9eDYL8f38+P33tu7ffODjrfsb6nDrPr2O37O+dX/P83/Sul9Vdd2XP9m6v238V637j79hWev+hgOt84uGFzsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACDE86As8Y2jN6tb9R+/e17pfVbXlsbHW/cve8pfW/ZE60rrPxWvb+K/az3jbrz/auv/yicOt+1zcVj1ySev+O6/8QOt+VdW6vYda9/+0rff39Ir1T7XuLxVe7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgxPOgL/M+qFw76BvN2+tCLWvfXvbH339Hyg0db9+kztGlj6/6OVT9q3a+qOvDIm1v3z259bev+FYdOtu7Pnj7Tur/Urdw/2bzfOl9VVf+c2Ny6/4v1+1r3X/2ND7fuLxVe7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAgxPCgL/CMp17+gkFfYd7u2X5/6/7YHU+37m+a3N66v+6WY637S9m/XrNy0FeYt9/u+vqgrzAv46dGW/f/9unrW/erqpYfPNp+xlI1PX5D+xmP3r2vdb/9d8T+ydb9pcKLHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEGB70BZ6xYnpm0FeYt42XnGnd3zT5gdb9Yzf8sHX/uok7W/erqtbuPdR+xmJ01UMnWveP7zzbul9VdcfEROv+FYdOtu5PfeYlrftv+OKx1v2qqsdf137EojW0aWPr/m93fb11v6pqy2Njrfvrbun/f5D582IHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGGB32BZyw/eLR1f/zUaOt+VdX+9Y+07p/7w5Wt+7tftrF1/8lXzLTuV1WtbT9hcZo9faZ1/+YDH2/dr6ra86UftO7v2n1r6/7K37fO16e2/qL3gKq6qza3n7FY/es1Kwd9hXl7y9rjrfvf2fum1v1uK3+/rP+M/ZPtZ5yPFzsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCCDsAgBDCDgAghLADAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACCHsAABCDA/6Agvl9Due137Glu+Pte4fv2Nf6363b9XooK/ABdowcbj9jJ2nbmvdf3TXxf39bHnsve1nXFZ/aT9jsbrqoROt+5tu3t66X1W169qftu6f2HZf6367bf1HvHX/K/sPOQ8vdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQQtgBAIQQdgAAIYQdAEAIYQcAEELYAQCEEHYAACGEHQBACGEHABBC2AEAhBB2AAAhhB0AQAhhBwAQYtnc3NzcoC8BAMD8ebEDAAgh7AAAQgg7AIAQwg4AIISwAwAIIewAAEIIOwCAEMIOACCEsAMACPFfksVe2vHkLCoAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T17:16:26.082295Z",
     "start_time": "2025-07-04T17:16:26.076754Z"
    }
   },
   "source": [
    "def feature_map(features):\n    # Apply Hadamard gates to all qubits to create an equal superposition state\n    for i in range(len(features[0])):\n        qml.Hadamard(i)\n\n    # Apply angle embeddings based on the feature values\n    for i in range(len(features)):\n        # For odd-indexed features, use Z-rotation in the angle embedding\n        if i % 2:\n            qml.AngleEmbedding(features=features[i], wires=range(8), rotation=\"Z\")\n        # For even-indexed features, use X-rotation in the angle embedding\n        else:\n            qml.AngleEmbedding(features=features[i], wires=range(8), rotation=\"X\")\n\n# Define the ansatz (quantum circuit ansatz) for parameterized quantum operations\ndef ansatz(params):\n    # Apply RY rotations with the first set of parameters\n    for i in range(8):\n        qml.RY(params[i], wires=i)\n\n    # Apply CNOT gates with adjacent qubits (cyclically connected) to create entanglement\n    for i in range(8):\n        qml.CNOT(wires=[(i - 1) % 8, (i) % 8])\n\n    # Apply RY rotations with the second set of parameters\n    for i in range(8):\n        qml.RY(params[i + 8], wires=i)\n\n    # Apply CNOT gates with qubits in reverse order (cyclically connected)\n    # to create additional entanglement\n    for i in range(8):\n        qml.CNOT(wires=[(8 - 2 - i) % 8, (8 - i - 1) % 8])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational approach\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-04T17:17:54.616059Z",
     "start_time": "2025-07-04T17:16:27.874204Z"
    }
   },
   "source": [
    "dev = qml.device(\"default.qubit\", wires=8)\n\n\n@qml.qnode(dev)\ndef circuit(params, features):\n    feature_map(features)\n    ansatz(params)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef variational_classifier(weights, bias, x):\n    return circuit(weights, x) + bias\n\n\ndef square_loss(labels, predictions):\n    return np.mean((labels - qml.math.stack(predictions)) ** 2)\n\n\ndef accuracy(labels, predictions):\n    acc = sum([np.sign(l) == np.sign(p) for l, p in zip(labels, predictions)])\n    acc = acc / len(labels)\n    return acc\n\n\ndef cost(params, X, Y):\n    predictions = [variational_classifier(params[\"weights\"], params[\"bias\"], x) for x in X]\n    return square_loss(Y, predictions)\n\n\ndef acc(params, X, Y):\n    predictions = [variational_classifier(params[\"weights\"], params[\"bias\"], x) for x in X]\n    return accuracy(Y, predictions)\n\n\nnp.random.seed(0)\nweights = 0.01 * np.random.randn(16)\nbias = jnp.array(0.0)\nparams = {\"weights\": weights, \"bias\": bias}\nopt = optax.adam(0.05)\nbatch_size = 7\nnum_batch = X_train.shape[0] // batch_size\nopt_state = opt.init(params)\nX_batched = X_train.reshape([-1, batch_size, 8, 8])\ny_batched = y_train.reshape([-1, batch_size])\n\n\n@jax.jit\ndef update_step_jit(i, args):\n    params, opt_state, data, targets, batch_no = args\n    _data = data[batch_no % num_batch]\n    _targets = targets[batch_no % num_batch]\n    _, grads = jax.value_and_grad(cost)(params, _data, _targets)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return (params, opt_state, data, targets, batch_no + 1)\n\n\n@jax.jit\ndef optimization_jit(params, data, targets):\n    opt_state = opt.init(params)\n    args = (params, opt_state, data, targets, 0)\n    (params, opt_state, _, _, _) = jax.lax.fori_loop(0, 200, update_step_jit, args)\n    return params\n\n\nparams = optimization_jit(params, X_batched, y_batched)\nvar_train_acc = acc(params, X_train, y_train)\nvar_test_acc = acc(params, X_test, y_test)\n\nprint(\"Training accuracy: \", var_train_acc)\nprint(\"Testing accuracy: \", var_test_acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7360248447204969\n",
      "Testing accuracy:  0.6944444444444444\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T17:18:24.996875Z",
     "start_time": "2025-07-04T17:18:24.988739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(params)\n",
    "# save the parameters to a file\n",
    "np.savez(\"variational_params.npz\", weights=params[\"weights\"], bias=params[\"bias\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias': Array(-0.22726513, dtype=float32), 'weights': Array([-1.0181081e-01,  7.3226631e-01,  4.9697268e-01, -3.5690758e-01,\n",
      "        4.6779245e-01, -6.3548438e-02, -1.4430081e+00, -1.3503240e+00,\n",
      "       -1.7692385e+00,  5.8935285e-01,  1.3474429e-01, -1.0524874e+00,\n",
      "        1.0132782e-01, -5.5722642e-01,  5.5856886e-05, -2.3709021e-04],      dtype=float32)}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T17:18:44.357091Z",
     "start_time": "2025-07-04T17:18:44.351121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "read_params = np.load(\"variational_params.npz\")\n",
    "weights = read_params[\"weights\"]\n",
    "bias = read_params[\"bias\"]\n",
    "print(\"Read weights: \", weights)\n",
    "print(\"Read bias: \", bias)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read weights:  [-1.0181081e-01  7.3226631e-01  4.9697268e-01 -3.5690758e-01\n",
      "  4.6779245e-01 -6.3548438e-02 -1.4430081e+00 -1.3503240e+00\n",
      " -1.7692385e+00  5.8935285e-01  1.3474429e-01 -1.0524874e+00\n",
      "  1.0132782e-01 -5.5722642e-01  5.5856886e-05 -2.3709021e-04]\n",
      "Read bias:  -0.22726513\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
