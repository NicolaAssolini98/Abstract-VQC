{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:03:49.334503Z",
     "start_time": "2025-07-06T14:03:49.326286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concrete_CCQC_digits import concrete_CCQC\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from skimage.transform import resize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def replace_values(arr, old_value, new_value):\n",
    "    return np.where(arr == old_value, new_value, arr)\n",
    "\n",
    "def resize_image(images, new_size=(4, 4)):\n",
    "    \"\"\"Resize a 2D image to a new size.\"\"\"\n",
    "    return np.array([resize(img, new_size, anti_aliasing=True) for img in images])\n",
    "\n",
    "num_qubits = 4 # 2,3,4 better choices\n",
    "class_0 = 2\n",
    "class_1 = 6\n",
    "\n",
    "# Load the digits dataset with features (X_digits) and labels (y_digits)\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "\n",
    "# Create a boolean mask to filter out only the samples where the label is 2 or 6\n",
    "filter_mask = np.isin(y_digits, [class_0, class_1])\n",
    "\n",
    "# Apply the filter mask to the features and labels to keep only the selected digits\n",
    "X_digits = X_digits[filter_mask]\n",
    "y_digits = y_digits[filter_mask]\n",
    "\n",
    "# Split the filtered dataset into training and testing sets with 10% of data reserved for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize the pixel values in the training and testing data\n",
    "# Convert each image from a 1D array to an 8x8 2D array, normalize pixel values, and scale them\n",
    "X_train = np.array([thing.reshape([8, 8]) / 16 * 2 * np.pi for thing in X_train])\n",
    "X_test = np.array([thing.reshape([8, 8]) / 16 * 2 * np.pi for thing in X_test])\n",
    "\n",
    "# Adjust the labels to be centered around 0 and scaled to be in the range -1 to 1\n",
    "# The original labels (2 and 6) are mapped to -1 and 1 respectively\n",
    "y_train = replace_values(y_train, class_0, -1)\n",
    "y_train = replace_values(y_train, class_1, 1)\n",
    "y_test = replace_values(y_test, class_0, -1)\n",
    "y_test = replace_values(y_test, class_1, 1)\n",
    "\n",
    "# Resize the images to 4x4\n",
    "X_train = resize_image(X_train, new_size=(num_qubits, num_qubits))\n",
    "X_test = resize_image(X_test, new_size=(num_qubits, num_qubits))\n",
    "\n",
    "# # TO PLOT NUMBERS:\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=3, layout=\"constrained\")\n",
    "# for i in range(2):\n",
    "#     for j in range(3):\n",
    "#       axes[i][j].matshow(X_train[2*(2*j+i)])\n",
    "#       axes[i][j].axis('off')\n",
    "# \n",
    "# fig.subplots_adjust(hspace=0.0)\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "weights = 0.01 * np.random.randn(num_qubits*2)\n",
    "bias = jnp.array(0.0)\n",
    "params = {\"weights\": weights, \"bias\": bias}\n",
    "opt = optax.adam(0.05)\n",
    "batch_size = 7\n",
    "num_batch = X_train.shape[0] // batch_size\n",
    "opt_state = opt.init(params)\n",
    "X_batched = X_train.reshape([-1, batch_size, num_qubits, num_qubits])\n",
    "y_batched = y_train.reshape([-1, batch_size])\n",
    "\n",
    "print(X_batched.shape)\n",
    "# Variational approach\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)\n",
    "\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    acc = sum([np.sign(l) == np.sign(p) for l, p in zip(labels, predictions)])\n",
    "    acc = acc / len(labels)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def cost(params, X, Y):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        vqc = concrete_CCQC(data=x, weights=params[\"weights\"], bias=params[\"bias\"])\n",
    "        predictions.append(vqc())\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "\n",
    "def acc(params, X, Y):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        vqc = concrete_CCQC(data=x, weights=params[\"weights\"], bias=params[\"bias\"])\n",
    "        predictions.append(vqc())\n",
    "    # predictions = [variational_classifier(params[\"weights\"], params[\"bias\"], x) for x in X]\n",
    "    return accuracy(Y, predictions)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_step_jit(i, args):\n",
    "    params, opt_state, data, targets, batch_no = args\n",
    "    _data = data[batch_no % num_batch]\n",
    "    _targets = targets[batch_no % num_batch]\n",
    "    _, grads = jax.value_and_grad(cost)(params, _data, _targets)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return (params, opt_state, data, targets, batch_no + 1)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def optimization_jit(params, data, targets):\n",
    "    opt_state = opt.init(params)\n",
    "    args = (params, opt_state, data, targets, 0)\n",
    "    (params, opt_state, _, _, _) = jax.lax.fori_loop(0, 200, update_step_jit, args)\n",
    "    return params\n",
    "\n",
    "\n",
    "params = optimization_jit(params, X_batched, y_batched)\n",
    "var_train_acc = acc(params, X_train, y_train)\n",
    "var_test_acc = acc(params, X_test, y_test)\n",
    "\n",
    "print(\"Training accuracy: \", var_train_acc)\n",
    "print(\"Testing accuracy: \", var_test_acc)\n",
    "print(params)\n",
    "# save the parameters to a file\n",
    "np.savez(f\"variational_params_{num_qubits}.npz\", weights=params[\"weights\"], bias=params[\"bias\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias': Array(-0.4678205, dtype=float32), 'weights': Array([-0.66718936, -0.9948772 , -1.3700426 , -0.0658858 , -0.00887135,\n",
      "       -0.0803882 ,  0.10717992, -0.18775432], dtype=float32)}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:06:04.148622Z",
     "start_time": "2025-07-06T14:06:04.139267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "read_params = np.load(f\"variational_params_{num_qubits}.npz\")\n",
    "weights = read_params[\"weights\"]\n",
    "bias = read_params[\"bias\"]\n",
    "print(\"Read weights: \", weights)\n",
    "print(\"Read bias: \", bias)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read weights:  [-0.66718936 -0.9948772  -1.3700426  -0.0658858  -0.00887135 -0.0803882\n",
      "  0.10717992 -0.18775432]\n",
      "Read bias:  -0.4678205\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Results training with different resizing\n",
    "\n",
    "4,4: \n",
    "\n",
    "Train:    0.8354037267080745\n",
    "\n",
    "Testing:  0.7777777777777778\n",
    "\n",
    "----------\n",
    "3, 3:\n",
    "\n",
    "Training accuracy:  0.8850931677018633\n",
    "\n",
    "Testing accuracy:  0.8333333333333334\n",
    "\n",
    "----------\n",
    "2, 2:\n",
    "\n",
    "Training accuracy:  0.8105590062111802\n",
    "\n",
    "Testing accuracy:  0.8888888888888888"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
